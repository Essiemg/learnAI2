{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23b7b2c8",
   "metadata": {},
   "source": [
    "# üéôÔ∏è Train Your Own TTS Voice Model\n",
    "**Tacotron2 Text-to-Speech Training on Google Colab**\n",
    "\n",
    "This notebook trains a voice model from scratch using the LJSpeech dataset.\n",
    "\n",
    "## Steps:\n",
    "1. Enable GPU runtime\n",
    "2. Install dependencies\n",
    "3. Upload/download dataset\n",
    "4. Train the model\n",
    "5. Download trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131957da",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Enable GPU\n",
    "Go to **Runtime ‚Üí Change runtime type ‚Üí GPU (T4)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aea3fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected! Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38bf1f2",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a9a43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch torchaudio librosa numpy scipy tqdm matplotlib soundfile\n",
    "\n",
    "import os\n",
    "\n",
    "# ============== DETECT PLATFORM (Kaggle first since it also has /content) ==============\n",
    "IS_KAGGLE = os.path.exists('/kaggle/input')  # More specific Kaggle check\n",
    "IS_COLAB = os.path.exists('/content') and not IS_KAGGLE  # Colab only if not Kaggle\n",
    "\n",
    "print(f\"üñ•Ô∏è Platform: {'Kaggle' if IS_KAGGLE else 'Colab' if IS_COLAB else 'Local'}\")\n",
    "\n",
    "# ============== SETUP STORAGE ==============\n",
    "if IS_KAGGLE:\n",
    "    # Kaggle: use working directory (persists during session)\n",
    "    CHECKPOINT_BASE = '/kaggle/working/tts_checkpoints'\n",
    "    \n",
    "    # Check if user uploaded checkpoint as a dataset\n",
    "    if os.path.exists('/kaggle/input/tts-checkpoint'):\n",
    "        print(\"üìÇ Found uploaded checkpoint dataset!\")\n",
    "        os.makedirs(f'{CHECKPOINT_BASE}/output', exist_ok=True)\n",
    "        import shutil\n",
    "        for f in ['best_model.pt', 'latest_model.pt']:\n",
    "            src = f'/kaggle/input/tts-checkpoint/{f}'\n",
    "            if os.path.exists(src):\n",
    "                shutil.copy(src, f'{CHECKPOINT_BASE}/output/{f}')\n",
    "                print(f\"  ‚úÖ Copied {f}\")\n",
    "    print(\"üíæ Checkpoints: /kaggle/working/tts_checkpoints\")\n",
    "    print(\"üì• Download from: Output tab (right side) after training\")\n",
    "\n",
    "elif IS_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    CHECKPOINT_BASE = '/content/drive/MyDrive/tts_checkpoints'\n",
    "    print(f\"üíæ Checkpoints: {CHECKPOINT_BASE}\")\n",
    "\n",
    "else:\n",
    "    CHECKPOINT_BASE = './tts_checkpoints'\n",
    "    print(f\"üíæ Checkpoints: {CHECKPOINT_BASE}\")\n",
    "\n",
    "os.makedirs(CHECKPOINT_BASE, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9948812",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Download LJSpeech Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ab7e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# ============== DATASET SETUP ==============\n",
    "if IS_KAGGLE and os.path.exists('/kaggle/input/ljspeech11'):\n",
    "    # Use Kaggle's built-in LJSpeech dataset (no download needed!)\n",
    "    print(\"‚úÖ Using Kaggle's LJSpeech dataset (instant!)\")\n",
    "    DATA_DIR = '/kaggle/input/ljspeech11/LJSpeech-1.1'\n",
    "    wavs_dir = Path(f'{DATA_DIR}/wavs')\n",
    "else:\n",
    "    # Download on Colab\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    url = \"https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\"\n",
    "    tar_path = \"data/LJSpeech-1.1.tar.bz2\"\n",
    "\n",
    "    if not os.path.exists(tar_path):\n",
    "        print(\"Downloading LJSpeech dataset (~2.6GB)...\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        total = int(response.headers.get('content-length', 0))\n",
    "        with open(tar_path, 'wb') as f:\n",
    "            with tqdm(total=total, unit='B', unit_scale=True) as pbar:\n",
    "                for chunk in response.iter_content(8192):\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(len(chunk))\n",
    "        print(\"Download complete!\")\n",
    "    else:\n",
    "        print(\"Dataset already downloaded\")\n",
    "\n",
    "    if not os.path.exists('data/LJSpeech-1.1'):\n",
    "        print(\"Extracting...\")\n",
    "        with tarfile.open(tar_path, 'r:bz2') as tar:\n",
    "            tar.extractall('data')\n",
    "        print(\"Extraction complete!\")\n",
    "    else:\n",
    "        print(\"Already extracted\")\n",
    "    \n",
    "    DATA_DIR = 'data/LJSpeech-1.1'\n",
    "    wavs_dir = Path(f'{DATA_DIR}/wavs')\n",
    "\n",
    "# Prepare metadata\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "available_wavs = {f.stem for f in wavs_dir.glob('*.wav')}\n",
    "print(f\"Found {len(available_wavs)} audio files\")\n",
    "\n",
    "entries = []\n",
    "with open(f'{DATA_DIR}/metadata.csv', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split('|')\n",
    "        if len(parts) >= 3 and parts[0] in available_wavs:\n",
    "            wav_path = wavs_dir / f\"{parts[0]}.wav\"\n",
    "            entries.append(f\"{wav_path}|{parts[2]}\")\n",
    "\n",
    "with open('data/processed/metadata.txt', 'w') as f:\n",
    "    f.write('\\n'.join(entries))\n",
    "\n",
    "print(f\"‚úÖ Prepared {len(entries)} samples for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6065991c",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Define Model Architecture (Tacotron2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7caaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Tuple, List, Optional, Dict\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============== CONFIGURATION ==============\n",
    "@dataclass\n",
    "class AudioConfig:\n",
    "    sample_rate: int = 22050\n",
    "    n_fft: int = 1024\n",
    "    hop_length: int = 256\n",
    "    win_length: int = 1024\n",
    "    n_mels: int = 80\n",
    "    mel_fmin: float = 0.0\n",
    "    mel_fmax: float = 8000.0\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    encoder_embedding_dim: int = 512\n",
    "    encoder_n_convolutions: int = 3\n",
    "    encoder_kernel_size: int = 5\n",
    "    attention_rnn_dim: int = 1024\n",
    "    attention_dim: int = 128\n",
    "    attention_location_n_filters: int = 32\n",
    "    attention_location_kernel_size: int = 31\n",
    "    decoder_rnn_dim: int = 1024\n",
    "    prenet_dim: int = 256\n",
    "    max_decoder_steps: int = 1000\n",
    "    gate_threshold: float = 0.5\n",
    "    p_attention_dropout: float = 0.1\n",
    "    p_decoder_dropout: float = 0.1\n",
    "    postnet_embedding_dim: int = 512\n",
    "    postnet_kernel_size: int = 5\n",
    "    postnet_n_convolutions: int = 5\n",
    "\n",
    "@dataclass\n",
    "class TTSConfig:\n",
    "    audio: AudioConfig = field(default_factory=AudioConfig)\n",
    "    model: ModelConfig = field(default_factory=ModelConfig)\n",
    "    characters: str = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789 .,!?'-\"\n",
    "    pad_token: str = \"_\"\n",
    "    batch_size: int = 16\n",
    "    learning_rate: float = 1e-3\n",
    "    weight_decay: float = 1e-6\n",
    "    epochs: int = 500\n",
    "    grad_clip_thresh: float = 1.0\n",
    "    data_path: str = \"data/processed/metadata.txt\"\n",
    "    checkpoint_dir: str = \"checkpoints\"\n",
    "    output_dir: str = \"output\"\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self.characters) + 1\n",
    "\n",
    "    @property\n",
    "    def n_mels(self) -> int:\n",
    "        return self.audio.n_mels\n",
    "\n",
    "print(\"‚úÖ Configuration defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b12202a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== TEXT & AUDIO PROCESSING ==============\n",
    "import soundfile as sf\n",
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self, config: TTSConfig):\n",
    "        self.char_to_idx = {config.pad_token: 0}\n",
    "        for i, char in enumerate(config.characters):\n",
    "            self.char_to_idx[char] = i + 1\n",
    "        self.idx_to_char = {v: k for k, v in self.char_to_idx.items()}\n",
    "\n",
    "    def text_to_sequence(self, text: str) -> List[int]:\n",
    "        return [self.char_to_idx[c] for c in text if c in self.char_to_idx]\n",
    "\n",
    "class AudioProcessor:\n",
    "    def __init__(self, config: AudioConfig):\n",
    "        self.config = config\n",
    "        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=config.sample_rate,\n",
    "            n_fft=config.n_fft,\n",
    "            hop_length=config.hop_length,\n",
    "            win_length=config.win_length,\n",
    "            n_mels=config.n_mels,\n",
    "            f_min=config.mel_fmin,\n",
    "            f_max=config.mel_fmax,\n",
    "        )\n",
    "\n",
    "    def load_audio(self, path: str) -> torch.Tensor:\n",
    "        # Use soundfile instead of torchaudio.load (avoids torchcodec issue)\n",
    "        audio, sr = sf.read(path)\n",
    "        waveform = torch.from_numpy(audio).float()\n",
    "        \n",
    "        # Handle stereo\n",
    "        if waveform.dim() > 1:\n",
    "            waveform = waveform.mean(dim=1)\n",
    "        \n",
    "        # Resample if needed\n",
    "        if sr != self.config.sample_rate:\n",
    "            waveform = torchaudio.functional.resample(waveform, sr, self.config.sample_rate)\n",
    "        \n",
    "        return waveform\n",
    "\n",
    "    def audio_to_mel(self, waveform: torch.Tensor) -> torch.Tensor:\n",
    "        if waveform.dim() == 1:\n",
    "            waveform = waveform.unsqueeze(0)\n",
    "        mel = self.mel_transform(waveform)\n",
    "        mel = torch.log(torch.clamp(mel, min=1e-5))\n",
    "        mel = (mel - mel.mean()) / (mel.std() + 1e-8)\n",
    "        return mel.squeeze(0)\n",
    "\n",
    "class LJSpeechDataset(Dataset):\n",
    "    def __init__(self, metadata_path: str, config: TTSConfig, max_samples: int = None):\n",
    "        self.text_processor = TextProcessor(config)\n",
    "        self.audio_processor = AudioProcessor(config.audio)\n",
    "        self.samples = []\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if max_samples and i >= max_samples:\n",
    "                    break\n",
    "                parts = line.strip().split('|')\n",
    "                if len(parts) >= 2 and Path(parts[0]).exists():\n",
    "                    self.samples.append((parts[0], parts[1]))\n",
    "        print(f\"Loaded {len(self.samples)} samples\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path, text = self.samples[idx]\n",
    "        text_seq = self.text_processor.text_to_sequence(text)\n",
    "        waveform = self.audio_processor.load_audio(audio_path)\n",
    "        mel = self.audio_processor.audio_to_mel(waveform)\n",
    "        return {\n",
    "            'text': torch.LongTensor(text_seq),\n",
    "            'text_length': len(text_seq),\n",
    "            'mel': mel,\n",
    "            'mel_length': mel.shape[1]\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    max_text = max(b['text_length'] for b in batch)\n",
    "    max_mel = max(b['mel_length'] for b in batch)\n",
    "    n_mels = batch[0]['mel'].shape[0]\n",
    "    B = len(batch)\n",
    "\n",
    "    text_padded = torch.zeros(B, max_text, dtype=torch.long)\n",
    "    mel_padded = torch.zeros(B, n_mels, max_mel)\n",
    "    gate_padded = torch.zeros(B, max_mel)\n",
    "    text_lengths = torch.LongTensor([b['text_length'] for b in batch])\n",
    "    mel_lengths = torch.LongTensor([b['mel_length'] for b in batch])\n",
    "\n",
    "    for i, b in enumerate(batch):\n",
    "        text_padded[i, :b['text_length']] = b['text']\n",
    "        mel_padded[i, :, :b['mel_length']] = b['mel']\n",
    "        gate_padded[i, b['mel_length']-1:] = 1.0\n",
    "\n",
    "    return {'text': text_padded, 'text_lengths': text_lengths,\n",
    "            'mel': mel_padded, 'mel_lengths': mel_lengths, 'gate': gate_padded}\n",
    "\n",
    "print(\"‚úÖ Data processing defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662b393e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== TACOTRON2 MODEL ==============\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(in_ch, out_ch, kernel, padding=(kernel-1)//2)\n",
    "        self.bn = nn.BatchNorm1d(out_ch)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(F.relu(self.bn(self.conv(x))))\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config: TTSConfig):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(config.vocab_size, config.model.encoder_embedding_dim)\n",
    "        self.convs = nn.ModuleList([\n",
    "            ConvBlock(config.model.encoder_embedding_dim, config.model.encoder_embedding_dim,\n",
    "                     config.model.encoder_kernel_size)\n",
    "            for _ in range(config.model.encoder_n_convolutions)\n",
    "        ])\n",
    "        self.lstm = nn.LSTM(config.model.encoder_embedding_dim,\n",
    "                           config.model.encoder_embedding_dim // 2,\n",
    "                           batch_first=True, bidirectional=True)\n",
    "\n",
    "    def forward(self, text, lengths):\n",
    "        x = self.embedding(text).transpose(1, 2)\n",
    "        for conv in self.convs:\n",
    "            x = conv(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = nn.utils.rnn.pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        x, _ = self.lstm(x)\n",
    "        x, _ = nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
    "        return x\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, config: ModelConfig, enc_dim):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(config.attention_rnn_dim, config.attention_dim, bias=False)\n",
    "        self.memory = nn.Linear(enc_dim, config.attention_dim, bias=False)\n",
    "        self.v = nn.Linear(config.attention_dim, 1, bias=False)\n",
    "        self.loc_conv = nn.Conv1d(2, config.attention_location_n_filters,\n",
    "                                  config.attention_location_kernel_size,\n",
    "                                  padding=(config.attention_location_kernel_size-1)//2)\n",
    "        self.loc_dense = nn.Linear(config.attention_location_n_filters, config.attention_dim, bias=False)\n",
    "\n",
    "    def forward(self, query, memory, attn_cat, mask=None):\n",
    "        q = self.query(query.unsqueeze(1))\n",
    "        k = self.memory(memory)\n",
    "        loc = self.loc_dense(self.loc_conv(attn_cat).transpose(1, 2))\n",
    "        e = self.v(torch.tanh(q + k + loc)).squeeze(-1)\n",
    "        if mask is not None:\n",
    "            e = e.masked_fill(mask, -float('inf'))\n",
    "        attn = F.softmax(e, dim=1)\n",
    "        ctx = torch.bmm(attn.unsqueeze(1), memory).squeeze(1)\n",
    "        return ctx, attn\n",
    "\n",
    "class Prenet(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([nn.Linear(in_dim, out_dim), nn.Linear(out_dim, out_dim)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = F.dropout(F.relu(layer(x)), 0.5, training=True)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, config: TTSConfig):\n",
    "        super().__init__()\n",
    "        self.n_mels = config.n_mels\n",
    "        self.max_steps = config.model.max_decoder_steps\n",
    "        self.gate_thresh = config.model.gate_threshold\n",
    "        enc_dim = config.model.encoder_embedding_dim\n",
    "\n",
    "        self.prenet = Prenet(config.n_mels, config.model.prenet_dim)\n",
    "        self.attn_rnn = nn.LSTMCell(config.model.prenet_dim + enc_dim, config.model.attention_rnn_dim)\n",
    "        self.attention = Attention(config.model, enc_dim)\n",
    "        self.dec_rnn = nn.LSTMCell(config.model.attention_rnn_dim + enc_dim, config.model.decoder_rnn_dim)\n",
    "        self.linear = nn.Linear(config.model.decoder_rnn_dim + enc_dim, config.n_mels)\n",
    "        self.gate = nn.Linear(config.model.decoder_rnn_dim + enc_dim, 1)\n",
    "        self.attn_drop = nn.Dropout(config.model.p_attention_dropout)\n",
    "        self.dec_drop = nn.Dropout(config.model.p_decoder_dropout)\n",
    "\n",
    "    def init_states(self, memory):\n",
    "        B, T, D = memory.shape\n",
    "        return (memory.new_zeros(B, self.attn_rnn.hidden_size),\n",
    "                memory.new_zeros(B, self.attn_rnn.hidden_size),\n",
    "                memory.new_zeros(B, self.dec_rnn.hidden_size),\n",
    "                memory.new_zeros(B, self.dec_rnn.hidden_size),\n",
    "                memory.new_zeros(B, T), memory.new_zeros(B, T),\n",
    "                memory.new_zeros(B, D))\n",
    "\n",
    "    def step(self, dec_in, states, memory, mask):\n",
    "        ah, ac, dh, dc, aw, awc, ctx = states\n",
    "        pre = self.prenet(dec_in)\n",
    "        ah, ac = self.attn_rnn(torch.cat([pre, ctx], 1), (ah, ac))\n",
    "        ah = self.attn_drop(ah)\n",
    "        ctx, aw = self.attention(ah, memory, torch.stack([aw, awc], 1), mask)\n",
    "        awc = awc + aw\n",
    "        dh, dc = self.dec_rnn(torch.cat([ah, ctx], 1), (dh, dc))\n",
    "        dh = self.dec_drop(dh)\n",
    "        out = torch.cat([dh, ctx], 1)\n",
    "        return self.linear(out), self.gate(out), (ah, ac, dh, dc, aw, awc, ctx)\n",
    "\n",
    "    def forward(self, memory, mel, lengths):\n",
    "        B, T = memory.shape[:2]\n",
    "        mask = torch.arange(T, device=memory.device).expand(B, T) >= lengths.unsqueeze(1)\n",
    "        go = memory.new_zeros(B, self.n_mels)\n",
    "        inputs = torch.cat([go.unsqueeze(1), mel.transpose(1,2)[:,:-1]], 1)\n",
    "        states = self.init_states(memory)\n",
    "        mels, gates = [], []\n",
    "        for t in range(inputs.size(1)):\n",
    "            m, g, states = self.step(inputs[:,t], states, memory, mask)\n",
    "            mels.append(m); gates.append(g)\n",
    "        return torch.stack(mels, 2), torch.cat(gates, 1)\n",
    "\n",
    "    def inference(self, memory, lengths=None):\n",
    "        B, T = memory.shape[:2]\n",
    "        mask = None\n",
    "        if lengths is not None:\n",
    "            mask = torch.arange(T, device=memory.device).expand(B, T) >= lengths.unsqueeze(1)\n",
    "        dec_in = memory.new_zeros(B, self.n_mels)\n",
    "        states = self.init_states(memory)\n",
    "        mels, gates = [], []\n",
    "        for _ in range(self.max_steps):\n",
    "            m, g, states = self.step(dec_in, states, memory, mask)\n",
    "            mels.append(m); gates.append(g)\n",
    "            if torch.sigmoid(g).item() > self.gate_thresh:\n",
    "                break\n",
    "            dec_in = m\n",
    "        return torch.stack(mels, 2), torch.cat(gates, 1)\n",
    "\n",
    "class PostNet(nn.Module):\n",
    "    def __init__(self, config: TTSConfig):\n",
    "        super().__init__()\n",
    "        ch = config.model.postnet_embedding_dim\n",
    "        k = config.model.postnet_kernel_size\n",
    "        n = config.model.postnet_n_convolutions\n",
    "        layers = [nn.Sequential(nn.Conv1d(config.n_mels, ch, k, padding=(k-1)//2),\n",
    "                               nn.BatchNorm1d(ch), nn.Tanh(), nn.Dropout(0.5))]\n",
    "        for _ in range(n - 2):\n",
    "            layers.append(nn.Sequential(nn.Conv1d(ch, ch, k, padding=(k-1)//2),\n",
    "                                       nn.BatchNorm1d(ch), nn.Tanh(), nn.Dropout(0.5)))\n",
    "        layers.append(nn.Sequential(nn.Conv1d(ch, config.n_mels, k, padding=(k-1)//2),\n",
    "                                   nn.BatchNorm1d(config.n_mels), nn.Dropout(0.5)))\n",
    "        self.convs = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for conv in self.convs:\n",
    "            x = conv(x)\n",
    "        return x\n",
    "\n",
    "class Tacotron2(nn.Module):\n",
    "    def __init__(self, config: TTSConfig):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(config)\n",
    "        self.decoder = Decoder(config)\n",
    "        self.postnet = PostNet(config)\n",
    "\n",
    "    def forward(self, text, text_len, mel, mel_len):\n",
    "        enc = self.encoder(text, text_len)\n",
    "        mel_out, gate_out = self.decoder(enc, mel, text_len)\n",
    "        mel_post = mel_out + self.postnet(mel_out)\n",
    "        return mel_out, mel_post, gate_out\n",
    "\n",
    "    def inference(self, text):\n",
    "        text_len = torch.LongTensor([text.size(1)]).to(text.device)\n",
    "        enc = self.encoder(text, text_len)\n",
    "        mel_out, gate_out = self.decoder.inference(enc, text_len)\n",
    "        mel_post = mel_out + self.postnet(mel_out)\n",
    "        return mel_post\n",
    "\n",
    "print(\"‚úÖ Tacotron2 model defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c029b6",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac81bbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== TRAINING ==============\n",
    "# Configuration\n",
    "config = TTSConfig()\n",
    "config.batch_size = 16  # Reduce to 8 if running on CPU\n",
    "config.learning_rate = 1e-3\n",
    "\n",
    "# Choose training mode\n",
    "QUICK_TEST = False  # Set to False for longer training\n",
    "RESUME_TRAINING = True  # Set to True to continue from checkpoint\n",
    "\n",
    "if QUICK_TEST:\n",
    "    MAX_SAMPLES = 1000\n",
    "    EPOCHS = 50\n",
    "    print(\"üöÄ QUICK TEST MODE: 1000 samples, 50 epochs\")\n",
    "else:\n",
    "    MAX_SAMPLES = 1000  # Keep same samples for consistency when resuming\n",
    "    EPOCHS = 200  # Train to 200 epochs total\n",
    "    print(\"üöÄ EXTENDED TRAINING: 1000 samples, 200 epochs\")\n",
    "\n",
    "# Setup device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "if device == 'cpu':\n",
    "    print(\"‚ö†Ô∏è Training on CPU - will be slower!\")\n",
    "    config.batch_size = 8\n",
    "\n",
    "# Use platform-aware checkpoint directories (set in cell 5)\n",
    "config.checkpoint_dir = f'{CHECKPOINT_BASE}/checkpoints'\n",
    "config.output_dir = f'{CHECKPOINT_BASE}/output'\n",
    "print(f\"üíæ Saving to: {config.output_dir}\")\n",
    "\n",
    "os.makedirs(config.checkpoint_dir, exist_ok=True)\n",
    "os.makedirs(config.output_dir, exist_ok=True)\n",
    "\n",
    "# Create dataset and model\n",
    "dataset = LJSpeechDataset(config.data_path, config, max_samples=MAX_SAMPLES)\n",
    "dataloader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True,\n",
    "                        collate_fn=collate_fn, num_workers=0)\n",
    "\n",
    "model = Tacotron2(config).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate,\n",
    "                            weight_decay=config.weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=10)\n",
    "\n",
    "# Resume from checkpoint if available\n",
    "start_epoch = 0\n",
    "best_loss = float('inf')\n",
    "\n",
    "latest_path = f\"{config.output_dir}/latest_model.pt\"\n",
    "best_path = f\"{config.output_dir}/best_model.pt\"\n",
    "\n",
    "if RESUME_TRAINING:\n",
    "    checkpoint_path = None\n",
    "    if os.path.exists(latest_path):\n",
    "        checkpoint_path = latest_path\n",
    "        print(f\"\\nüìÇ Found latest checkpoint\")\n",
    "    elif os.path.exists(best_path):\n",
    "        checkpoint_path = best_path\n",
    "        print(f\"\\nüìÇ Found best checkpoint\")\n",
    "    \n",
    "    if checkpoint_path:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        if 'optimizer_state_dict' in checkpoint:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint.get('epoch', 0) + 1\n",
    "        best_loss = checkpoint.get('loss', float('inf'))\n",
    "        print(f\"‚úÖ Resumed from epoch {start_epoch}, loss: {best_loss:.4f}\")\n",
    "    else:\n",
    "        print(\"\\nüÜï No checkpoint found, starting fresh...\")\n",
    "else:\n",
    "    print(\"\\nüÜï Starting fresh training...\")\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model parameters: {params:,}\")\n",
    "print(f\"Batches per epoch: {len(dataloader)}\")\n",
    "print(f\"Training epochs: {start_epoch} ‚Üí {EPOCHS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a274d540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop (continues from start_epoch)\n",
    "losses = []\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    for batch in pbar:\n",
    "        text = batch['text'].to(device)\n",
    "        text_len = batch['text_lengths'].to(device)\n",
    "        mel = batch['mel'].to(device)\n",
    "        mel_len = batch['mel_lengths'].to(device)\n",
    "        gate = batch['gate'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        mel_out, mel_post, gate_out = model(text, text_len, mel, mel_len)\n",
    "\n",
    "        loss = mse_loss(mel_out, mel) + mse_loss(mel_post, mel) + bce_loss(gate_out, gate)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip_thresh)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    losses.append(avg_loss)\n",
    "    scheduler.step(avg_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Loss: {avg_loss:.4f} - LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "    # Save best model\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': best_loss,\n",
    "            'config': config\n",
    "        }, f\"{config.output_dir}/best_model.pt\")\n",
    "        print(f\"  ‚úÖ Saved best model (loss: {best_loss:.4f})\")\n",
    "\n",
    "    # ALWAYS save latest checkpoint (so we never lose progress!)\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': avg_loss,\n",
    "        'config': config\n",
    "    }, f\"{config.output_dir}/latest_model.pt\")\n",
    "\n",
    "    # Also save numbered checkpoint every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss\n",
    "        }, f\"{config.checkpoint_dir}/checkpoint_epoch_{epoch+1}.pt\")\n",
    "        print(f\"  üíæ Checkpoint saved: epoch {epoch+1}\")\n",
    "\n",
    "print(\"\\nüéâ Training complete!\")\n",
    "print(f\"Best loss: {best_loss:.4f}\")\n",
    "print(f\"Model saved to: {config.output_dir}/best_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dbfd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True)\n",
    "plt.savefig(f\"{config.output_dir}/loss_curve.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f4d123",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a474f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal\n",
    "import numpy as np\n",
    "from IPython.display import Audio\n",
    "\n",
    "# Load best model (weights_only=False needed for custom config class)\n",
    "checkpoint = torch.load(f\"{config.output_dir}/best_model.pt\", weights_only=False)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Text to synthesize\n",
    "text = \"Hello, I am your AI tutor. How can I help you today?\"\n",
    "\n",
    "# Convert text to sequence\n",
    "text_proc = TextProcessor(config)\n",
    "seq = text_proc.text_to_sequence(text)\n",
    "text_tensor = torch.LongTensor([seq]).to(device)\n",
    "\n",
    "# Generate mel spectrogram\n",
    "with torch.no_grad():\n",
    "    mel = model.inference(text_tensor)\n",
    "\n",
    "print(f\"Generated mel shape: {mel.shape}\")\n",
    "\n",
    "# ============== USE HIFI-GAN VOCODER (Much better quality!) ==============\n",
    "# Install and load pretrained HiFi-GAN\n",
    "try:\n",
    "    import librosa\n",
    "    \n",
    "    # Try to use pretrained HiFi-GAN from torchaudio\n",
    "    hifigan_available = False\n",
    "    try:\n",
    "        bundle = torchaudio.pipelines.HIFIGAN_VOCODER\n",
    "        hifigan = bundle.get_vocoder().to(device)\n",
    "        hifigan_available = True\n",
    "        print(\"‚úÖ Using HiFi-GAN vocoder (high quality)\")\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è HiFi-GAN not available, using Griffin-Lim (lower quality)\")\n",
    "    \n",
    "    if hifigan_available:\n",
    "        # HiFi-GAN expects specific mel format - denormalize our mel\n",
    "        mel_for_vocoder = mel.squeeze(0)  # Remove batch dim\n",
    "        \n",
    "        # Denormalize (reverse our normalization)\n",
    "        mel_denorm = mel_for_vocoder * 2.5  # Scale up\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            audio_tensor = hifigan(mel_denorm)\n",
    "        \n",
    "        audio = audio_tensor.squeeze().cpu().numpy()\n",
    "    else:\n",
    "        # Fallback to improved Griffin-Lim\n",
    "        def griffin_lim_improved(mel, config, n_iter=100):\n",
    "            mel = mel.squeeze().cpu().numpy()\n",
    "            \n",
    "            # Denormalize\n",
    "            mel = mel * 2.5 + 4  # Reverse normalization approximately\n",
    "            mel = np.exp(mel)\n",
    "            mel = np.clip(mel, 0, 1000)  # Prevent explosion\n",
    "            \n",
    "            # Create mel filterbank\n",
    "            mel_basis = torchaudio.functional.melscale_fbanks(\n",
    "                n_freqs=config.audio.n_fft // 2 + 1,\n",
    "                f_min=config.audio.mel_fmin,\n",
    "                f_max=config.audio.mel_fmax,\n",
    "                n_mels=config.audio.n_mels,\n",
    "                sample_rate=config.audio.sample_rate\n",
    "            ).numpy().T\n",
    "            \n",
    "            # Inverse mel to linear\n",
    "            linear = np.maximum(1e-10, np.dot(np.linalg.pinv(mel_basis), mel))\n",
    "            \n",
    "            # Griffin-Lim with more iterations\n",
    "            angles = np.exp(2j * np.pi * np.random.rand(*linear.shape))\n",
    "            for i in range(n_iter):\n",
    "                full = linear * angles\n",
    "                audio = scipy.signal.istft(\n",
    "                    full, \n",
    "                    fs=config.audio.sample_rate,\n",
    "                    nperseg=config.audio.win_length,\n",
    "                    noverlap=config.audio.win_length - config.audio.hop_length\n",
    "                )[1]\n",
    "                \n",
    "                if i < n_iter - 1:\n",
    "                    _, _, new_spec = scipy.signal.stft(\n",
    "                        audio, \n",
    "                        fs=config.audio.sample_rate,\n",
    "                        nperseg=config.audio.win_length,\n",
    "                        noverlap=config.audio.win_length - config.audio.hop_length\n",
    "                    )\n",
    "                    angles = np.exp(1j * np.angle(new_spec[:linear.shape[0], :]))\n",
    "            \n",
    "            return audio.astype(np.float32)\n",
    "        \n",
    "        audio = griffin_lim_improved(mel, config)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Using basic Griffin-Lim...\")\n",
    "    # Basic fallback\n",
    "    mel_np = mel.squeeze().cpu().numpy()\n",
    "    mel_np = np.exp(mel_np)\n",
    "    audio = librosa.feature.inverse.mel_to_audio(\n",
    "        mel_np,\n",
    "        sr=config.audio.sample_rate,\n",
    "        n_fft=config.audio.n_fft,\n",
    "        hop_length=config.audio.hop_length,\n",
    "        win_length=config.audio.win_length,\n",
    "        n_iter=100\n",
    "    )\n",
    "\n",
    "# Normalize audio\n",
    "audio = audio / (np.abs(audio).max() + 1e-8) * 0.9\n",
    "\n",
    "print(f\"Generated {len(audio) / config.audio.sample_rate:.2f} seconds of audio\")\n",
    "print(\"\\n‚ö†Ô∏è NOTE: For clear speech, train for 200+ epochs with QUICK_TEST = False\")\n",
    "\n",
    "# Play audio\n",
    "Audio(audio, rate=config.audio.sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b93b2c3",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Download Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0336ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the trained model\n",
    "import shutil\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    # On Kaggle: copy to /kaggle/working so it appears in Output tab\n",
    "    output_file = f\"{config.output_dir}/best_model.pt\"\n",
    "    kaggle_output = \"/kaggle/working/best_model.pt\"\n",
    "    if os.path.exists(output_file):\n",
    "        shutil.copy(output_file, kaggle_output)\n",
    "        print(f\"‚úÖ Model copied to: {kaggle_output}\")\n",
    "        print(\"üì• Download from the 'Output' tab on the right side of the notebook!\")\n",
    "    else:\n",
    "        print(\"‚ùå No model found to download\")\n",
    "elif IS_COLAB:\n",
    "    from google.colab import files\n",
    "    files.download(f\"{config.output_dir}/best_model.pt\")\n",
    "    print(\"‚úÖ Model downloaded!\")\n",
    "else:\n",
    "    print(f\"‚úÖ Model saved to: {config.output_dir}/best_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113af30e",
   "metadata": {},
   "source": [
    "## üìù Notes for Your Professor\n",
    "\n",
    "This notebook implements:\n",
    "\n",
    "1. **Tacotron2 Architecture** - State-of-the-art sequence-to-sequence TTS\n",
    "   - Encoder: Character embedding + 3 conv layers + BiLSTM\n",
    "   - Attention: Location-sensitive attention mechanism\n",
    "   - Decoder: Autoregressive mel spectrogram generation\n",
    "   - PostNet: 5 conv layers to refine output\n",
    "\n",
    "2. **Training from Scratch**\n",
    "   - Public LJSpeech dataset (24 hours, 13,100 samples)\n",
    "   - Custom data pipeline for audio processing\n",
    "   - Mel spectrogram features (80 mel bins)\n",
    "\n",
    "3. **Complete ML Pipeline**\n",
    "   - Data loading and preprocessing\n",
    "   - Model definition\n",
    "   - Training loop with checkpointing\n",
    "   - Loss computation (MSE + BCE)\n",
    "   - Inference and audio synthesis"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
